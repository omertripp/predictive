\section{Technical Overview}\label{sec:overview}

In this section, we walk the reader through a detailed technical description of our approach based on the example in Figure \ref{fig:running}. 
%
As input, we assume (i) a program $P$ as well as (ii) a trace of $P$ recorded during a dynamic execution.

% in Static Single Assignment (SSA) form, such that every variable is defined exactly once.


\subsection{Preliminaries}
We first introduce some notation used throughout this paper. Intuitively, a trace is a sequence of events recorded during the observed run. An event, $e=<t, id, inst>$, is a concrete representation of the runtime execution of a static instruction $inst$, such that (i) $t$ refers to the thread that issues the event $e$ (denoted $t^e$); (ii) $id$ is a unique identifier of the event (unless spcified otherwise, we use the index of an event in the trace as its id); and (iii) $inst$ is a static three-address instruction (involving at most three operands, which modern compilers commonly support).

For this paper, we utilize the instructions listed in Table~\ref{Ta:syntax}. When the variable does not appear on the left hand of an equation, such as $y$ in $x.f=y$, it may refer to a variable, a constant or event object creation expression $new (...)$.  The $bop$ stands for the binary operator, which may refer to $+, -, *, /, \%, \wedge, \vee$ in the assignment, or refer to $<, >, =, \wedge, \vee$ in the branch. The target of the branch event is not important in our scope, therefore, we may abbreviate the branch instruction as the boolean expression afterwards. The listed instructions suffice to represent all trace events of interest. This is because a concrete finite execution trace can be reduced to a straight-line loop-free call-free path program  (argument passing of the method call is modeled as assignments and the virtual call resolution is modeled as branches). This standard form of simplification preserves all the data-race-related information contained in the original trace. 

\begin{table}
	\begin{center}
		\begin{tabular}{rcl}
			\multicolumn{1}{l}{{\tt s} $::=$} & & \\
			{\tt y = x.f} & $|$ & {\bf (heapr)} \\ 
			{\tt x.f = y}  & $|$ & {\bf (heapw)} \\ %\ $|$\ {\tt x.f = $c$}\
			{\tt z = x $bop$ y}\  & $|$ & {\bf (assign)} \\ %$|$\ {\tt z = $c$} $|$ {\tt z = new()}
			{\tt if (x $bop$  y) goto ...} & $|$ &  {\bf (branch)} \\
			{\tt lock(l)}\ $|$\ {\tt unlock(l)}  & $|$& {\bf (sync)} \\
			{\tt fork(t)}\ $|$\ {\tt join(t)}$|$ {\tt begin(t)}\ $|$\ {\tt end(t)} &  & {\bf (thread)}
		\end{tabular}
	\end{center}
	\caption{\label{Ta:syntax}Instruction Types}
\end{table}


{\bf Trace Representation \ } In the following, we assume that trace $\tau$ is represented as $\tau=<\Gamma , \{\tau_{t_1}, \tau_{t_2}, \dots \tau_{t_n} \}, O, \theta>$, where
\begin{itemize}
\item  $\Gamma$ refers to the set of threads involved in the trace, which include $t_1$, $t_2$, \dots, $t_n$;
\item   $\{\tau_{t_1}, \tau_{t_2}, \dots \tau_{t_n} \}$ denotes the event sequences produced by each thread;
\item  $O$ is a function that assigns  to every event an integer value that denotes its scheduling order: $e_i$ is scheduled earlier than $e_j$ iff $O(e_i)<O(e_j)$; and
\item  $\theta$ is a function that maps every live variable to a value. 
%For example, given an event $e$ executing the instruction $x=y+z$, the mapping recorded in the trace may be $\theta(x)=3$,  $\theta(y)=2$,  $\theta(z)=1$.  
\end{itemize}
To ensure the integrity of $\theta$ as well simplify downstream encoding steps performed by the analysis already at the syntactic level, we rewrite the raw input trace into SSA form, such that each variable is defined exactly once.


%\item $map$ is a mapping from live expressions at the current event to their value (and thus a partial mapping from expressions to values). Live expressions include program variables, object fields, boolean expressions, etc. We use the notation $\lsyn exp \rsyn^e$ to retrieve the value associated with expression $exp$ at $e$.
%Specially,  we also store the heap location value  $\lsyn \overline{x.f} \rsyn^e$ for a field $x.f$. Note the difference between $\lsyn \overline{x.f} \rsyn^e$ and $\lsyn x.f \rsyn^e$, which represent the location and the value stored in it respectively. The location value $\lsyn \overline{x.f} \rsyn^e$ is computed as a pair $(\lsyn x \rsyn^e, f)$.


%We introduce the projection operation ``$\downarrow$'' to facilitate the reasoning of the trace:
%$\tau\downarrow_{t}$ contains only the (ordered) events from the thread $t$; $\tau\downarrow_{\ell}$ contains the events involving the location $\ell$; $\tau\downarrow_{\leq id}$ denotes the prefix of $e_{id}$, i.e., the events preceding the event $e_{id}$; $\tau\downarrow_{\geq id}$ denotes the events after $e_{id}$; $\tau\downarrow_{>=id1\wedge <=id2}$ denotes the events between $e_{id1}$ and $e_{id2}$.

%\begin{itemize}
%\item projection operation  : 
%%\item sets: $\mathcal{T}$ denotes the set of threads involved;  $\mathcal{L}$ denotes the set of memory locations involved; 
%%$\mathcal{SV}$ denotes the set of shared variables, which reference the shared locations, the shared location $l$ can be judged by counting the number of threads in $\tau\downarrow_{\ell}$;  $\mathcal{E}$ denotes the set of events involved.
%\end{itemize}






%We make use of the following helper functions:
%\begin{itemize}
%	\item ${\sf proj}\ t\ i$ projects trace $t$ onto all transitions involving thread $i$.
%	\item $t[k]$ obtains the $k$th transition within trace $t$.
%	\item ${\sf index}\ t\ \tau$ retrieves the index, or offset, of transition $\tau$ within trace $t$. When simply writing
%	${\sf index}\ \tau$ (while omitting the trace parameter) we refer to the index of $\tau$ within the original trace. 
%	\item ${\sf pre}\ t\ \tau$ is the prefix of trace $t$ preceding transition $\tau$. For the suffix beyond $\tau$, we 
%	use ${\sf post}\ t\ \tau$. Finally, ${\sf bet}\ t\ \tau_1\ \tau_2$ returns the transitions delimited by $\tau_1$ and $\tau_2$.
%\end{itemize}

%Throughout this paper, we assume a standard operational semantics, which defines (i) a mapping ${\sf thr}$ between
%execution threads, each having a unique identifier $i \in \mathbb{N}$, and their respective code, as well as (ii) per-thread stack and shared heap memory. 

%The code executed by a given thread follows the syntax in Table \ref{Ta:syntax}. The text of a program is a sequence of zero or more method declarations, as given in the definition of symbol {\tt p}. Methods accept zero or more arguments $\overline{{\tt x}}$, have a body ${\tt s}$, and may have a return value (which we leave implicit). For simplicity, we avoid from static typing as well as virtual methods. The body of a method consists of the core grammar for symbol {\tt s}. We avoid from specifying syntax checking rules, as the grammar is fully standard.

%For simplicity, we assume that in the starting state each thread points to a parameter-free method. In this way, we can simply assume an empty starting state (i.e., an empty heap), and eliminate complexities such as user-provided inputs and initialization of arguments with default values. These extensions are of course possible, and in practice we handle these cases, but reflecting them in the formalism would result in needless complications.

%As is standard, we assume an interleaved semantics of concurrency. A \emph{transition}, or \emph{event}, is of the form
%$\sigma \stackrel{i / {\tt s}}{\longrightarrow} \sigma'$, denoting that thread $i$ took an evaluation step
%in prestate $\sigma$, wherein atomic
%statement {\tt s} was executed, which resulted in poststate $\sigma'$. We refer to a sequence of transitions from
%the starting state to either an exceptional state (e.g., due to null dereference) or a state where all the threads have 
%reduced their respective code to $\epsilon$ as a \emph{trace}.


%TODO this section is inconsistent with others. change it!
\subsection{Constraint System}
The predictive analysis takes the program and a trace as the input.  Suppose it starts with the following trace of the program in  Figure \ref{fig:running}:  $e_1 e_2 e_3 e_4 e_5$, where the line number is used as the event id (subscript). A potential race is between the pair, $(e_2, e_5)$, as they access the shared variable $x$ from different threads and one is a write. 

Given the trace, $\tau=<\Gamma , \{\tau_{t_1}, \tau_{t_2}, \dots \tau_{t_n} \}, O, \theta>$, the predictive analysis computes a new trace,  $\tau'=<\Gamma , \{\tau_{t_1}, \tau_{t_2}, \dots \tau_{t_n} \}, O', \theta'>$. $\tau'$ preserves the same event sequence for each thread, i.e., $T_1$ still executes $e_1 e_2 e_3$ and $T_2$ still executes $e_4 e_5$, but it reschedules the events from different threads so that the potential race pair runs concurrently, i.e., $e_2$ and $e_5$ run concurrently. When the schedule changes, the variables may get different values. Therefore, we need to compute both the new schedule $O'$ and the new value mapping $\theta'$, in order to witness a race. To guarantee that $\tau'$ is a feasible trace of the program $P$, the computation needs to respect a set of constraints, which include the value constraints, control flow constraints and synchronization constraints~\footnote{We omit the synchronization constraints in this paper, which are already well explained in existing techniques~\cite{yannis, pecan}.}. We explain them briefly in the following. The full details are explained in Section~\ref{sec:relax1} and the feasibility guarantee is explained in Section~\ref{sec:guarantee}.




%The local accesses are not used in this example and therefore omitted.






%We begin with an explanation of the encoding process. The resulting formula is provided in Figure \ref{fig:encoding}. We describe the conjuncts comprising the formula one by one.

%\begin{figure*}
%	\begin{center}
%$$
%	\begin{array}{rcl}
%	& \left( O_1 < O_2 < O_3 \wedge O_4 < O_5 \wedge O_4 < O_7 \right) & \textbf{(program order)} \\
%\bigwedge & \left( W_{\tt x}^0 = 0 \wedge W_{\tt y}^0 = 0 \wedge W_{\tt z}^0 = 3 \wedge W_{\tt y}^1 = 3 \wedge 
%	W_{\tt x}^2 = 1 \wedge W_{\tt y}^3 = 5 \right) & \textbf{(variable definitions)} \\
%\bigwedge & \left(		(R_{\tt y}^4=W_{\tt y}^0 \wedge O_4 < O_1) \vee	
%(R_{\tt y}^4=W_{\tt y}^1 \wedge O_1 < O_4 < O_3) \vee
%(R_{\tt y}^4=W_{\tt y}^3 \wedge O_3 < O_4)
%		\right) & \textbf{(thread interference)} \\
%\bigwedge & R_{\tt y}^4 > 2 \equiv {\bf true} & \textbf{(path conditions)} \\
%\bigwedge & t^2 \neq t^5 \wedge (2\ writes\ {\tt x} \vee 5\ writes\ {\tt x}) \wedge O_2 = O_5 & \textbf{(race condition)}
%	\end{array} 
%$$
%\end{center}
%\caption{\label{fig:encoding}\tool\ encoding of the trace in Figure \ref{fig:running} as a constraint system}
%\end{figure*}


We first preprocess the trace as follows. We introduce symbols to replace the shared accesses in the instructions, e.g., We use $W^{id}_{x}$/$R^{id}_{x}$ to replace the write/read of the shared variable $x$ by the event $e_{id}$. %The rationale is explained in Section~\ref{sec:relax1}.

{\bf Race Condition\ } The potential race pair involves two accesses of the same shared location from different threads, where at least one access is a write. Given any potential race pair, e.g., ($e_2$, $e_5$), it is a real race if and only if the two accesses can occur at the same time (race condition), which is captured by the race condition constraint: 
$O'(e_2) = O'(e_5)$.
The race condition constraint--- together with the other constraints --- guarantees the feasibility of the predicted race if a solution is found for the overall constraint system.

{\bf Intra-thread Constraints\ } The predictive analysis needs to preserve the same event sequence for each thread, which further requires 

\begin{itemize}
\item {\bf Control Flow Constraints\ } Each thread takes the same control flows (or branch decisions) to reproduce the events.
 For the running example (ignoring the statements in red), this yields: $\theta'(R_{\tt y}^4) > 2 \equiv {\bf true}$.
That is, in the predicted trace $\tau'$, the value of variable {\tt y} read at $e_4$ should be greater than $2$. 


\item {\bf Intra-thread Order Constraints\ } The events should follow the same thread-local order as in the original trace. This constraint is imposed by the fact that the two runs share the same instruction sequence and take the same control flows. 
We obtain the  formula: $O'(e_1) < O'(e_2) < O'(e_3) \wedge O'(e_4) < O'(e_5)$.
That is, in the predicted trace $\tau'$, the three events from $T_1$ are ordered in the same way as in $\tau$, and similarly, the branch event $e_4$ in $T_2$ still executes before the event $e_5$ inside the branch body.

\item {\bf Intra-thread Value Constraints\ } The events should respect the value constraints imposed by each instruction.
 For our running example, we obtain:
$$
\begin{array}{l}
	\theta'(W_{\tt x}^0) = 0 \wedge \theta'(W_{\tt y}^0) = 0 \wedge \theta'(W_{\tt y}^1) = 3 \\ 
	\wedge	\theta'(W_{\tt x}^2) = 1 \wedge \theta'(W_{\tt y}^3) = 5
	\end{array}
$$
As an example, $\theta'(W_{\tt y}^3)= 5$ denotes that the predicted run still assigns the value 5 to the variable {\tt y}  at event $e_3$.
\end{itemize}

%The next set of constraints, denoted ${\tt w}^i=k$ for local variables and $W_{\tt w}^i=k$ for shared variables, capture variable definitions: Variable {\tt w} is assigned value $k$ at statement $i$.


%Indeed, this constraint is satisfied by the assignments to {\tt y} both at line {\tt 1} and at line {\tt 3}.


{\bf Inter-thread Value Constraints\ } The inter-thread value constraints capture what writes the read events read from and under what scheduling condition the value flow occurs.  The resulting formula for our example is
$$
\begin{array}{lcl}
	& & (\theta'(R_{\tt y}^4)=\theta'(W_{\tt y}^0) \wedge O'(e_4) < O'(e_1) ) \\
& \bigvee &
	(\theta'(R_{\tt y}^4)=\theta'(W_{\tt y}^1) \wedge O'(e_1) < O'(e_4) < O'(e_3)) \\
& \bigvee &
	(\theta'(R_{\tt y}^4)=\theta'(W_{\tt y}^3) \wedge O'(e_3) < O'(e_4))
\end{array}
$$    
Notice, importantly, that the formula associates the value constraints with the scheduling order constraints. 
 As an example, 
$\theta'(R_{\tt y}^4)=\theta'(W_{\tt y}^1) \wedge O'(e_1) < O'(e_4) < O'(e_3)$ means that, in the predicted run, the read $e_4$ reads from the write $e_1$, under the condition that $e_4$ happens after $e_1$ and no other  writes (such as $e_3$) interleave them.

The formuals from the different encoding steps are conjoined and sent to the off-the-shelf solver such as {\sf z3}.  The race under analysis is real if the solver returns a solution, which includes the scheduling order and the values for the variables. 



{\bf Unexplored Branches\ } Beyond the encoding steps so far, which focus on the given trace, we can often encode constraints along unexplored branches. In our running example, this is essential to discover the race between lines {\tt 2} and {\tt 7}. We explore the unexecuted branch statically and collect constraints during the exploration. We then form a disjunction  of the original constraints and the collected constraints to include more possible  paths. The solver may pick up the unexplored branch to detect a race whenever it is necessary. Back to our example, the event $e_7$ is in the unexplored branch, our solver is able to identify the race between $e_7$ and $e_2$ after encoding the unexplored branch. 

For the racy events, $e_2$ and $e_7$, we first specify the race condition constraint  $\theta'(R^4_y)\leq 2\rightarrow O'_{e_2}=O'(e_7)$. Note that the race condition constraint associates the race with the branch condition that enables each event.  Also we need to encode (1) the intra-thread constraints, such as the  order constraints $O'(e_4)<O'(e_5)\wedge O'(e_4)<O'(e_7)$, and (2) the inter-thread constraints, which remain the same as before. With these constraints, the solver confirms the race pair, ($e_2$, $e_7$). More details are explained in Section~\ref{sec:relax2}.





<<<<<<< HEAD
=======
{\bf Constraint Solving\ } The formuals from the different encoding steps are conjoined and sent to an off-the-shelf solver such as {\sf z3}.  The race under analysis is guaranteed to be real if the solver returns a solution, which includes the scheduling order and the values for the variables. 
>>>>>>> 1102fa63b8d224e0b01c2d08fc7143fcedbf9c42

{\bf Highlights \ } In this example, our analysis finds two more pairs of races beyond existing analyses~\cite{yannis, jeff}: $(e_2, e_5)$ and $(e_2, e_7)$.
For the first race, we do not require $e_4$ to read from $e_3$ to retain the original value, as in approaches. Rather, we allow $e_4$ to read from $e_1$ while still preserving the feasibility of the branch. This relaxation allows the events after $e_4$ to run concurrently with the events before $e_3$, leading to the detection of the first race.  For the second race, we cover unexecuted branches and ask the solver to compute a feasible schedule that exercises them. The neighboring branches are likely to contain racy events if the executed branches contain racy events.

 
%In particular, the solution discloses a feasible trace that gives rise to the race at hand. The trace is identified uniquely via the order enforced in the solution over the variables $O_i$, which represent scheduling order. 
