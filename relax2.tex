\section{Exploration of Unexecuted Branches}~\label{sec:relax2}
We now switch to the second feature of \tool, which is its ability to reason about neighboring branches that are not executed in the original trace $\tau$.
At the high level, we replace the constraints that model the executed branch to the constraints that conservatively model the possible executions in both branches.
We leverage the static analysis to  realize this relaxation. 

We start with a branch event in the trace, without loss of generality, suppose its true branch is taken, we need to explore the false branch and add constraints to model it. As the static analysis has some known challenges, we make several simplifying assumptions: (1) the unexplored branch should not contain loops or recursions because the loop bound may depend on some shared variable which is statically unknown; (2) every base reference variable should be fully resolved to a concrete object, as required by our analysis (Section~\ref{sec:relax1}). We apply dataflow analysis (reaching definitions) to judge: If the base reference depends on some shared read in the explored branch, of which the value is statically unknown, it cannot be statically resolved to an object; If the base reference depends on multiple definitions from different paths, it cannot be statically resolved to an object. Otherwise, it can be resolved. (3) We assume the unexplored branch does not contain array access because the array accesses often use the index expressions that are statically unknown. Similarly, as the collections often use array for implementation, we assume they are also absent. (4) We inline the method calls (for two levels) in the unexplored branch, similar to the base object, we assume the caller object can be fully resolved statically.  If any of the above assumptions is violated, we  avoid exploring the unexplored branches and adopt the purely dynamic solution in Section~\ref[sec:relax1}. 
We use the example in Figure~\ref{fig:running2}


 
 For presentation, we refer to the branch structure as, $B=<header, B_{if}, B_{else}>$, which includes the header instruction and two branches. Without loss of generality, we assume the original trace takes the branch $B_{if}$. Our running example is shown in Figure~\ref{fig:path}. 

{\bf Events \ }. We traverse the control flow graph of the branch $B$ and produce an event, $<t, id, inst>$, to represent the possible execution of each instruction $inst$. Here, $t$ is the same as that of the root branch event as the exploration is conducted thread-locally, $id$ is a unique value different from existing ids. Specifically,  we maintain an increasing counter to assign the id following the traversal order. For this example, we simply use the line number as the event id.
As we assume no loops and recursions, we establish the one-to-one mapping between each instruction $inst$ and the event $e_{inst}$.  We may use the two terms interchangeably.


{\bf SSA Form\ }  We apply SSA transformation to the code of the branch structure and then update the instruction stored in the corresponding event. Our SSA transformation is similar to the standard SSA transformation but needs minor adaptations. 
\begin{itemize}
\item {\bf Local primitive variables\ }  For the variable $z$ defined in an instruction $inst$ inside the branch, our SSA transformation replaces it as $z^{id}$, where $id$ comes from the event $e_{inst}$. Some local variables are defined out of the branch structure, e.g., the variable $x$ is defined at line 5 in Figure~\ref{fig:path}. We look up the SSA encoding history up to the branch event to find the symbol that replaces the variable most recently and use the symbol instead. For example, the variable $x$ is replaced to $x^5$ at the event $e_5$ according to the history, we simple replace the uses of $x$ as $x^5$. In addition, the SSA transformation may introduce Phi node in the form of $y=Phi(y_1, C_1, y_2, C_2, \dots y_i, C_i)$, meaning that $y$ is equal to $y_i$ in the condition $C_i$.
\item {\bf Local Heap Accesses\ } Given the heap access of $x.f$, remember that the base variable $x$ is fully resolved to a concrete object $o$, we replace it as a local variable $l_{o.f}$ before the SSA transformation and treat it as a local primitive variable during the SSA transformation. 
\item {\bf Shared Heap Accesses\ } The SSA transformation does not encode the shared heap accesses. Similar to Section~\ref{sec:relax1}, we introduce the special symbols to denote the shared read/write.
\end{itemize}

The SSA form for the branch at line 6 is as follows, where the base variable $w$ at line 13 is replaced to an object $o2$ and the local heap access $w.f$ is encoded as a local primitive variable.


\begin{figure}
\centering
\begin{tabular}{l}
 {\tt 6: if($x^5$<10)}  \\ %{\tt if($x^5$<10)}\\
{\tt 7: $W^7_{S}$=0;}   \\%{\tt $W^7_{S}$=0;}\\
 {{\tt 8:  else}}   \\ %{\color{Gray}{\tt if($x^5\geq$10)}}\\
 {{\tt 9: \ \ $z^9$=$R^{9}_{o1.f}$;}}  \\ %{\color{Gray}{\tt $z^{9}$=$R^{9}_{o1.f}$;}}\\
 {{\tt 10: \ if($x^5$<$z^9$)}}  \\%{\color{Gray}{\tt if($x^5<z^{9}$)}}\\
 {{\tt 11: \ \ $W^{11}_{S}$=0;}} \\%{\color{Gray}{\tt $W^{11}_{S}$=0;}}\\ 
 {{\tt 12: \ else }}  \\%{\color{Gray}{\tt if($x^5\geq z^{10}$)}}\\
 {{\tt 13: \ \ $W^{13}_{S}$=$l^{id}_{o2.f}$; }}   \\%{\color{Gray}{\tt $W^{13}_{S}$=$l^{id}_{o2.f}$;}}\\
\end{tabular}
\caption{Running Example (shared variables are in bold font). }
\label{fig:pathssa}
\end{figure}






{\bf Constraints\ } During the traversal, we also record the shared access events.  For the set of accesses $S_1$ of the location $\ell$, we also find the accesses $S_2$ of it from other threads. We assert the feasibility of each race pair $\in S_1 \times S_2$ by specifying constraints and checking them.

{\bf Race Condition Constraint\ }  The first constraint is the race condition constraint, e.g., $O'_{4}=O'(e_11)$ specifies that the event at line 4 may form a race with the event at line 11, which is not executed in the original trace.

{\bf Intra-thread Control Flow Constraint \ } Different from the strategy in Section~\ref{sec:relax1}, we do not assert the branch the predicted run will take. Instead, we assert that all branches inside the branch structure may be taken, which is trivially $true$. The solver automatically reasons about which branch should be taken, as indicated in the solution. Note that only one of the two branches can be taken because the conditions for them contradict.

{\bf Intra-thread Value Constraint \ } Intra-thread value constraint is the core constraint that realizes the exploration different branches. It asserts the value constraints imposed by the {\sf assign/heapr/heapw} events take effect only when the guarding condition is satisfied. More formally, we have


\[ cons(n) = \left\{ 
  \begin{array}{l l}
  (\bigwedge_{n_1\in n_{if}}{  (expr(header_n) \rightarrow  cons(n_1))}) \\
  \bigwedge (\bigwedge_{n_2\in n_{else}}  { (\neg expr(header_n) \rightarrow  cons(n_2))}) \\
   \quad  \quad  \quad \quad  \quad  \quad  \quad  \quad  \quad  \text{if n is a branch} \\
  equation(n)      \\                                  
   \quad  \quad  \quad \quad  \quad  \quad \quad  \quad  \quad  \text{otherwise} \\ 
  \end{array} \right.\]
Here, $expr(header)$ is the helper function that returns the condition associated with the header. $equation(n)$ returns the equation inside each event, which imposes the value constraint.  In the first case when the instruction $n$ is a branch, we assert the value constraint imposed by each instruction in the if-branch can take effect only when the if-branch is taken, i.e., when $expr(header_n)$ is evaluated as true,  and the value constraint imposed by instruction in the else-branch can take effect only when the else-branch is taken. In the second case, when the instruction $n$ is a normal instruction that contains the equation, we assert the equation as the value constraint. Note that the constraint is defined recursively because a branch  may contain another branch structure.


Consider the example in Figure~\ref{fig:pathssa},  the constraint is,
$$
\begin{array}{l}
&(\theta'(x^5)<10 \rightarrow \theta'(W^7_{S})=0) \wedge (\theta'(x^5)\geq 10 \rightarrow \theta'(z^9)=\theta'(R^{9}_{o1.f}) 
&\wedge  (\theta'(x^5)\geq 10 \rightarrow  
& \ \ \ \ \  \ \ (\theta'(x^5)<\theta'(z^9) \rightarrow \theta'(W^{11}_{S})=0 
& \ \ \ \ \  \ \ \wedge \theta'(x^5)\geq \theta'(z^9) \rightarrow \theta'(W^{13}_{S})=\theta'(l^{id}_{o2.f}))
&)\\ 
\end{array}
$$

{\bf Intra-thread Order Constraints\ } The intra-thread order constraint captures the program order imposed by the branch structure and the instruction sequence. More formally, for each branch $n$, we have,

\[ cons(n) = \left\{ 
  \begin{array}{l l}
  (\bigwedge_{n_1\in n_{if}}{O'(header_n)<O'(n_1)<O'(end_n)}) \\
   (\bigwedge_{n_1 and n_3 are adjacent in n_{if}}{O'(n_1)<O'(n_3)}) \\
   (\bigwedge_{n_2\in n_{else}}{O'(header_n)<O'(n_2)<O'(end_n)}) \\
    (\bigwedge_{n_2 and n_4 are adjacent in n_{else}}{O'(n_2)<O'(n_4)}) \\
  \end{array} \right.\]
Here, $end_n$ is a node that denotes the first event after the branch.  The order constraints are imposed by the program, and therefore, are unconditionally valid. That is why we do not specify the order constraints under specific branch conditions. 

{\bf Inter-thread value constraints\ } The inter-thread value constraints are identical to those in Section~\ref{sec:relax1}. Interesting, we do not need to specify the value constraints under the specific branch condition. The underlying reason is that, the intra-thread value constraint already encodes the branch condition information, if the branch condition is not satisfied at runtime, the value constraint does not take effect and the read of a shared variable cannot propagate its value downstream, even if it is assigned to a value. 





\begin{figure}
\centering
\begin{tabular}{ll|l}
\multicolumn{3}{c}{{\tt {\bf P} = new();//$o1$}} \\
\multicolumn{1}{c}{$T_1$} & \multicolumn{1}{c}{$T_2$}  &  \multicolumn{1}{c}{$Trace$}\\
{\tt 1: {\bf P}.f=100; } &  & {\tt $W^{1}_{o1.f}$=100;}\\
{\tt 2: {\bf Q}=101; } &  & {\tt $W^{2}_{Q}$=101;}\\
{\tt 3: {\bf Q}=1; } & & {\tt $W^{3}_{Q}$=1;} \\
{\tt 4: {\bf S}=1; } & & {\tt $W^{4}_{S}$=1;} \\
& {\tt 5: x={\bf Q};} & {\tt $x^5$=$R^5_{Q}$;}\\
& {\tt 6: if(x<10)} & {\tt if($x^5$<10)}\\
& {\tt 7: \ {\bf S}=0;}  & {\tt $W^7_{S}$=0;}\\
& {\color{Gray}{\tt 8:  else}} &  \\ %{\color{Gray}{\tt if($x^5\geq$10)}}\\
& {\color{Gray}{\tt 9: \ \ z={\bf P}.f;}} & \\ %{\color{Gray}{\tt $z^{9}$=$R^{9}_{o1.f}$;}}\\
& {\color{Gray}{\tt 10: \ if(x<z)}} & \\%{\color{Gray}{\tt if($x^5<z^{9}$)}}\\
& {\color{Gray}{\tt 11: \ \ {\bf S}=0;}}& \\%{\color{Gray}{\tt $W^{11}_{S}$=0;}}\\ 
& {\color{Gray}{\tt 12: \ else }}  & \\%{\color{Gray}{\tt if($x^5\geq z^{10}$)}}\\
& {\color{Gray}{\tt 13: \ \ {\bf S}=w.f; }}  & \\%{\color{Gray}{\tt $W^{13}_{S}$=$l^{id}_{o2.f}$;}}\\
\end{tabular}
\caption{Running Example (shared variables are in bold font). }
\label{fig:path}
\end{figure}
%
%Consider the example in Figure~\ref{fig:path}, only the if-branch of $T_2$ is executed in the original trace $\tau$.
%Following the rules in Section~\ref{sec:relax1},   we derive a set of constraints to find the new trace $\tau'$ to witness the race pair ($e_5$, $e_8$). We list those specific to if-branch below.
%
%\begin{figure}
%	\begin{center}
%$$
%	\begin{array}{rcl}
%		&  O'_{e_5}=O'_{e_8} & \textbf{(race condition)} \\
%			& \theta'(x^6)<10 & \textbf{(Intra-thread control flow constraint)} \\
%	&  O'_{e_7}<O'_{e_8} & \textbf{(Intra-thread order constraint)} \\
%		& \theta'(W^8_S)=0 & \textbf{(Intra-thread value constraint)} \\
%	\end{array} 
%$$
%\end{center}
%caption{\label{fig:pathencode}
%\end{figure}
%
%
%We  replace these constraints to another set of constraints that account for both if-branch and else-branch from the event $e_7$. We refer to the branch event $e_7$ as the root branch event, denoted as $e_{root}$.
%
%
%
%First, we conduct the DFS traversal of the branch structure and produce an event, $<t, id, inst>$, to represent the potential execution of each instruction $inst$. Here, $t$ is the same as that of the root branch event as the exploration is conducted thread-locally, $id$ is a unique value different from existing ids. Specifically,  
% we maintain an increasing counter to assign the id following the traversal order. For this example, we simply use the line number as the event id.
%
%
%
%similar to Section~\ref{sec:relax1}, we need the SSA form of 





%%TODO | and projection, abuse the terms?
%As shown in the following, suppose the symbolic execution  $symEngine(\tau, e)$ returns the single-thread traces starting from the branch $e$, which now takes a different decision. The resultant trace is computed by first removing the events thread-locally after $e$ (inclusively) and then appending one of the single-thread traces that represents a path in the unexplored branch of $e$. The negation $neg(e)$ of $e$, which is the same as $e$ except the evaluation result of the boolean expression differs, is also appended.
%
%
%
%\begin{algorithmic}[3]
%\For {$e: \tau$}
%  \If {$type(e)=branch$}
%    \State $S=symEngine(\tau, e)$
%	\For {$\tau': S$}
%	\State $\tau_r=\tau-\tau\downarrow_{t^e \wedge \geq e}$
%	\State $\tau_r=\tau_r.replace(e, neg(e) + \tau')$
%	\EndFor
% \EndIf
%\EndFor
%\end{algorithmic}
%
%
%{\bf Symbolic Execution\ } Our symbolic execution is realized on top of the dataflow analysis, which generates events for each instruction within the unexplored branch.
%The symbolic execution has known limitations in reasoning about the schedules and loops (and recursion). Therefore, our analysis assumes all the events in the unexplored branch happen atomically at the branch event $e$ without interleavings from other threads, as illustrated also in the above algorithm (line 6). In this way, our analysis adopts the sequential reasoning. \tool\ will reschedule the events based on the relaxation. As for loop (or recursion), the analysis terminates the current data flow immediately after one iteration, i.e., we expand the loop for only once. Otherwise, the analysis terminates the data flow normally if the flow goes out of the scope of the unexplored branch.
%
%The analysis starts at the event $e$ (exclusively), with the state $\sigma$.  Initially, the state includes the runtime heap value for the base objects that are already resolved. During the symbolic execution, the state is updated to maintain the heap value and also the boolean expression values for branches. The values for other variables are not used indeed and therefore treated as symbolic. The update of the state is realized through the   standard kill/gen rules for three basic heap instructions. In the object creation, which we separate from the local assignment, we assign a unique integer to represent the newly created object and store it into the state $\sigma$. For field access, $\sigma[x.f]$ actually denotes $\sigma[o.f]$, where  $o$ is the base object resolved in the state. 
%These kill/gen rules take effect only when $x.f$  is not of the primitive type.
%
%The analysis also generates the events, where the heap values are stored in the map of the event for later use. For the branch, we store its boolean evaluation result.
%As the boolean result is available only after the branch takes the decision, we delay the generation of the branch event until the first event after the branch. 
%
%
%
%
%\begin{table*}
%\centering
%%\begin{small}
%\begin{tabular}{l|c|c}
%\multicolumn{1}{c|}{Operation} & {Kill} & {Gen} \\
%\hline
%{\tt $y$=$new (...)$} 			&  $\sigma[y] \mapsto \star$		&  $\sigma[y] \mapsto newI$, $<t, cnt, y=newI, \bot>$\\
%{\tt $y$=$x.f$} 			&   $\sigma[y]\mapsto \star$			&  $\sigma[y] \mapsto \sigma[x.f]$, $<t, cnt, y=x.f, \lsyn x \rsyn=\sigma[x]>$  \\
%{\tt $x.f=y$} & $\sigma[x.f] \mapsto \star$		&  $\sigma[x.f] \mapsto \sigma[y]$, $<t, cnt, x.f=y, \lsyn x \rsyn=\sigma[x]>$ \\
%{\tt $z=x\  bop\  y$} & 		&  $<t, cnt, z=x\  bop\  y, \bot>$ \\
%{\tt $if(x<y)$} & 	  &  $<t, cnt, if(x<y), \lsyn x<y \rsyn=bool>$\\
%\end{tabular}
%\caption{\label{table:killgen}Dataflow analysis}
%%\end{small}
%\end{table*}




%Beyond relaxing flow dependencies, \tool\ is also capable of exploring code branches that were not executed in the original trace. This is achieved via a symbolic representation of the input trace. Given branching event $e_b$,
%\begin{enumerate}
%	\item model the branching condition symbolically to execute along the negation of the original branch;
%	\item apply depth-first search (DFS) to uncover all execution suffixes under the unexplored branch (which may itself contain branching statements); and
%	\item for each suffix $t^s$,
%	\begin{enumerate}
%		\item truncate the original trace at $e_b$ yielding prefix $t_s$; and
%		\item concatenate $t_s$ with the negation of $e_b$ followed by $t^s$.
%	\end{enumerate}
%\end{enumerate}
%Constraint solving is applied to each of the resulting traces analogously to the original trace.

%We emphasize that exploration of new execution paths is subject to all the known limitations of symbolic execution, including in particular loop structures and object allocation. \tool\ currently fails if (i) the unexplored branch containts loops or (ii) there are object references that cannot be fully resolved at the branching point. In case of failure, \tool\ moves on to other branches. We demonstrate in Section \ref{sec:eval} that despite these limitations, the increase in coverage thanks to exploration of new paths is significant.