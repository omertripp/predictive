\section{Relaxation of Flow Dependencies}~\label{sec:relax1}
The main goal of our analysis is to derive, given an original trace $\tau$, a trace $\tau'$ over the same set of events in $\tau$, which has a new scheduling  of the events (the order of events inside each thread should remain unchanged) and a new mapping from variables to values in the events. Our first relaxation comes from the insight that we allow the variables to read different values than in the original trace even if such variables are used to determine the control flow, while existing approaches enforce them to read the same values as in the original trace. As a result, our relaxation allows more schedules, which are likely to expose more races.
 
Not every scheduling or every mapping  leads to a new feasible trace $\tau'$. Therefore, we need to compute scheduling and mapping instances that lead to feasible traces. We achieve this via constraint solving. We explain the details of our technique in the following.


% % 
% TODO: Explain the format of the raw trace in impl.tex.
% %


%We begin by describing the first relaxation, whereby the dependence structure defined by the original trace is potentially violated while retaining soundness.

\subsection{SSA Form of the Trace}~\label{sec:ssa}
First of all, we rewrite the trace into SSA form, such that every variable in defined exactly once in the trace. This requirement is a prerequisite for the computation of the mapping, in which each variable is mapped to exactly one value. Another side effect is that the def/use chains become explicit in the SSA form, which simplifies the following analysis steps.

\tool\ handles the local assignment and heap accesses differently.
\begin{itemize}
\item {\bf Local Assignment\ } The SSA form for local assignment resembles the SSA form of static instructions in compiler optimization, except that the loops and recursions are fully resolved in a concrete trace, obviating the need for the {\sf Phi} node. More concretely, we replace the variable $v$ defined in an event, as well as the following uses of the definition, to a new variable $v^{id}$, where $id$ is the unique id of the event. The uniqueness of the id guarantees that no two events define the same variable, i.e., each variable is defined exactly once. Note that uses of a definition can be computed easily by scanning the trace suffix for accesses to the same variable name before its next definition. 
\item {\bf Heap accesses\ } The accesses of the local heap locations and the accesses of shared locations play different roles in our analysis. We therefore introduce different SSA form for them. 
\begin{itemize}
\item {\bf Local Heap Accesses\ } The accesses of local heap locations behave similarly to the local assignment as both the definition and uses belong to a single thread. Therefore we model them as local assignments: We introduce a fresh local variable {\tt $l_{o.f }$} to replace each definition and  the corresponding uses of the location of $o.f$ ($o$ denotes an object referenced by the variable $x$ in $x.f$).  
%An assumption here is the uses and the definition still refer to the same base object $o$ in the predicted run, which is enforced through the constraint specification %(Section~\ref{sec:relax1}).
\item  {\bf Shared Heap Accesses\ } The accesses of shared locations are more complex. Each shared read may read from one of multiple writes that update the shared location under different schedules (e.g., the read access to {\tt y} at line 4 in Figure \ref{fig:running}, which can either obtain the definition before line 1 or at line 1 or at line 3).   Given the write $x.f=y$ or the read $y=x.f$, we introduce two symbols, $W^{id}_{\tt o.f }$, which denotes the write access by the event $id$ to the  shared heap location  $o.f$  ($o$ denotes an object referenced by the variable $x$ in $x.f$), and $R^{id}_{\tt  o.f }$,  which denotes the read access  by the event $id$ to the location $o.f$. 
\end{itemize}
We use the heap location such as $o.f$ in the the symbolic form of heap accesses. An underlying assumption is that the heap location remains unchanged in our predictive analysis, which we ensure through additional constraints (Section~\ref{sec:relax1}).
\end{itemize}


% Similar to the local heap accesses, an assumption for the correlation is that the writes and reads still refer to the same base object $o$ in the predicted run, which we guarantee by specifying additional constraints (Section~\ref{sec:relax1}). 

% We could encode the question of shared versus local accesses as additional constraints (requiring that a variable be accessed by more than one thread), but that would be more complicated and less efficient.


%\paragraph{Basic Encoding: Local Accesses}
%
%The fundamental encoding transformation is to induce Static Single Assignment (SSA) form on the raw trace, such that
%a variable is defined exactly once. In this way, , and encoding of trace events as constraints is simplified. As an illustration, trace
%\begin{quote}
%	{\tt 1: x=1; 2: x<3; 3: x=3;} \\
%	{\tt 4: y=1;} \\
%	{\tt 5: z=x+y}
%\end{quote}
%becomes
%\begin{quote}
%	{\tt 1: x$^1$=1; 2: x$^1$<3; 3: x$^3$=3;} \\
%	{\tt 4: y$^4$=1;} \\
%	{\tt 5: z$^5$=x$^3$+y$^4$}
%\end{quote}
%(For readability, we version variables according to the line number of their definition.)



%%TODO update the intro+moti, make sure the same style
\begin{figure}
\centering
\begin{tabular}{ll}
\multicolumn{2}{c}{{\tt {\bf x} = 0; {\bf y} = 0;}} \\
\hline
\multicolumn{1}{c}{$T_1$} & \multicolumn{1}{c}{$T_2$} \\
\hline
{\tt 1: s=0; } & \\
{\tt 2: for(i=1;i<2;i++)} & \\
{\tt 3: \ \ \ s+=i;} & \\
{\tt 4: {\bf y} = s;} & \\
& {\tt 5: if ({\bf y} > 2)} \\
& {\tt 6:~~print({\bf x});} \\	
\end{tabular}
\caption{Running Example (shared variables are in bold font). }
\label{fig:running2}
\end{figure}

 
\begin{figure}
\centering
\begin{tabular}{l|l}
\hline
\multicolumn{1}{c}{$Trace$} & \multicolumn{1}{c}{$SSA \ form \ of\   Trace$} \\
\hline
{\tt 0: {\bf x}=0} &  {\tt 0: $W^0_x$=0}    \\
{\tt 1: {\bf y}=0} &   {\tt 1: $W^1_y$=0}   \\
{\tt 2: s=0} &  {\tt 2: $s^2$=0}   \\
{\tt 3: i=1} &     {\tt 3: $i^3$=1}   \\
{\tt 4: i<2} &    {\tt 4: $i^3$<3} \\
{\tt 5: s=s+i} & {\tt 5: $s^5$=$s^2$+$i^3$}   \\
{\tt 6: i=2} &       {\tt 6: $i^6$=2}  \\
{\tt 7: i<2} &      {\tt 7: $i^6$<3}  \\
{\tt 8: {\bf y} = s;} &  {\tt 8: $W^{8}_y$ = $s^5$;}  \\
{\tt 9: {\bf y} > 2}  &    {\tt 9: $R^{9}_y$ > 2} \\
{\tt 10: print({\bf x});} &  {\tt 10: print($R^{10}_x$);}  \\
\end{tabular}
\caption{Trace}
\label{fig:t4running2}
\end{figure}

Figure~\ref{fig:t4running2} exemplifies a trace and its SSA form, where the trace is generated from the program in Figure~\ref{fig:running2}.
Each label in  Figure~\ref{fig:t4running2} on the left hand denotes the id of the event. As seen, event $e_5$ uses the variable $s^2$ defined at event $e_2$ and defines the variable $s^5$, which is used at $e_8$. Reads and writes of shared variables are denoted in the special form explained above (e.g.: $W^{8}_y$ and  $R^{9}_y$).





\subsection{Constraint System}~\label{sec:constraints}
Based on the SSA form of the trace, we build the constraints to compute a new trace with the new schedule and new mapping of variables to values.
To model the schedule, we introduce the order variable $O_{id}$ for each event $e_{id}$. Given two events $e_i, e_j$ from two different threads, $O_{e_i}<O_{e_j}$ means that $e_i$ is scheduled before $e_j$. We omit the synchronization constraints intentionally, as they are well explained in all existing predictive analysis techniques~\cite{yannis, pldi14}.




%Having explained how the trace is encoded, we now describe in detail how constraints are derived from the trace, such that any permutation considered by the analysis is guaranteed to represent a feasible execution schedule.

{\bf Race Condition\ } Following the standard definition, pair ($e_i$, $e_j$) of events forms a race iff (1) $e_i$ and $e_j$ are accesses of the same location $\ell$ by different threads, (2) at least one of them writes to $\ell$, and (3) $e_i$ and $e_j$ run concurrently. We refer to the candidate pair throughout this section as (candidate) \emph{racy events}.  

We first identify all candidate pairs of events that access the same location from different threads (and at least one is a write). We then check each pair  separately.   The checking is achieved by encoding all necessary constraints and invoking a constraint solver.  The first constraint asserts the feasibility of the concurrent execution of the racy events:
$$
O_{i} = O_{j}
$$



{\bf Intra-thread Constraints\ } Given the candidate race pair, $e_i$ and $e_j$, suppose $e_i$ is after $e_j$ in the original trace $\tau$. The predictive analysis reschedules the events in the prefix of $e_i$, i.e., prior to $e_i$, so that $e_i$ and $e_j$ can run concurrently. The predictive analysis by design requires that each thread should follow the same event sequence prior to $e$ as in the original run.  The preservation of the event sequence for each thread requires the following intra-thread constraints:

\begin{itemize}
\item {\bf Control Flow Constraints\ } The branches in the predicted run should take the same decisions as in the original trace so that each thread reproduces the same set of events.  Specifically, we only need to reason about the branches prior to $e_i$.

More formally,  we require that 
$$
\bigwedge_{e_k \in \tau\downarrow_{\leq i} \wedge  type^{e_k}=branch.\ }  inst^{e_k} 	\equiv \lsyn inst \rsyn^{e_k}
$$ 
where  the branch event $e_k$ is used as a boolean expression. For example, given the branch {\tt if(x<y)\dots}, the constraint is in the form of $(x<y)\equiv {\bf true}$ assuming the branch evaluates to {\bf true} in the original trace. The constraint specifies that the boolean expression in the predicted trace should be evaluated to the same boolean value as in the original trace. Importantly, Unlike existing analyses \cite{yannis,pldi14}, we do not pose the requirement that the values flowing into branching statements remain the same, but adopt the relaxed requirement that the evaluation of branching expressions remains the same. For example, suppose  the branch event $x<y$ takes the value $1<2$ in the original trace. Existing analyses require the same values for the variables $x$ and $y$ in the predicted run, while we allow  the variables flowing into the branch event to assume other values, such as $3<4$, as long as the branch expression retains the same truth value. 
\item {\bf Intra-thread Order Constraints\ } The events in the sequence should follow the same order as in the original trace. More formally, we require that
$$
\begin{array}{rl}
\forall e_m, e_n \in \tau\downarrow_{\leq i}, s.t., & t^{e_m} = t^{e_n}. \\
& m < n\  \Rightarrow O_{m} < O_{n}
\end{array}
$$ 
This constraint specifies that two events from the same thread should follow the same order as reflected by the ids of the events. Note that $m$ in $e_m$ denotes its id. Again, we only consider the events in the prefix of $e_i$.
\item {\bf Intra-thread Data Flow Constraints\ } The mapping of local variables should not contradict the data flow semantics of each instruction.   More formally, we require that
$$
\bigwedge_{e_k \in \tau\downarrow_{\leq i} \wedge type(e_k)=assign|heapr|heapw}\
	inst^{e_k}
$$
where $inst^{e_k}$, such as $x=y+z$ or $R^{id}_{x}=y$, captures the constraint over the values of the variables imposed by the instruction. For example, $x=y+z$ is not satisfied if the solver maps the three variables to $3, 4, 5$ respectively. Again we consider only the events in the prefix of  $e_i$. Specially, for the instruction involving object creation, $x=new (...)$, we encode the object as a unique integer that denotes its heap address, calculated by $System.identityHashcode(x)$ at runtime.
\end{itemize}



%Rescheduling through Relaxation of Flow Dependencies
{\bf Inter-thread Data Flow Constraints for Relaxation\ } We now move to the first novel feature of \tool, which is its ability to explore execution schedules that depart from the value flow exhibited in the original trace. More precisely, \tool\ is able to relax value flow dependencies in the original trace:  a read access may read a
different value from other write events, as long as the read value enforces feasibility.
 This is strictly beyond the coverage potential of existing predictive analyses, which restrict trace transformations to ones where any read access must read the same  value (often from the same write event) as in the original trace. 

To ensure feasibility under relaxation of flow dependencies, we need to secure the flow between the read/write with the execution schedule. 
For example, in Figure \ref{fig:running} for the read access to ${\tt y}$ at $e_4$ to obtain the value assigned to ${\tt y}$ at $e_1$, we need a schedule where 
$e_4$ happens after $e_1$ and other writes such as $e_3$ do not interleave them. ($e_3$ can only happen after $e_4$ in this case.)


% $R_{\tt y}^4={\tt y}^1 \wedge O_1 < O_4 < O_3$ specifies that in a schedule , the $R_{\tt y}^4={\tt y}^1$.

%TODO l and ell
In general, the constraint formula, given read $R^{m}_{\ell}$ of location $\ell$ at the event $e_m$ with set ${\cal W}$ of matching write events (i.e., events including write access to $\ell$), takes the following form:
$$
\begin{array}{rll}
\bigvee_{e_n \in {\cal W}} &  & (R^{m}_{\ell} = W^{n}_{\ell}) \\
&		\bigwedge 	&  O_n < O_m \\
&		\bigwedge_{e_p \in {\cal W} \setminus \{ e_n \}} & (O_p < O_n \vee O_m < O_p)
\end{array}
$$
This disjunctive formula iterates over all matching write events, and demands for each that (i) it occurs prior to the read event ($O_n < O_m$) and (ii) all other write events either occur before ($O_p < O_n$) it or after the read event
($O_m < O_p$).

An important concern that arises due to relaxation of flow dependencies is that heap accesses may change their meaning, i.e., they involve different base objects and no longer match with each other. As an illustration, we refer to Figure \ref{fig:heapAccess}. While the read at the event $e_7$ appears to match the write at $e_5$, this is conditioned on the read at $e_6$ being linked to the assignment at $e_4$. Suppose  the event $e_6$ reads from $e_3$ due to the relaxation. Then $e_7$ and $e_5$ no longer share the same base object (or the same location). Even worse, we do not know which events $e_7$ matches, because the base object is no longer known.



\begin{figure}
	\centering
	\begin{tabular}{ll}
		\hline
		\multicolumn{1}{c}{$T_1$} & \multicolumn{1}{c}{$T_2$} \\
		\hline
		{\tt 1: $x^1$ = new();// creates o1} & \\
		{\tt 2: $x^2$= new();// creates o2} & \\
		{\tt 3: $W^3_{y}$ = $x^1$;} & \\
		{\tt 4: $W^4_{y}$ = $x^2$;} & \\
		{\tt 5: $W^5_{o2.f}$ = 5;} & \\	
		& {\tt 6: $z^6$ = $R^6_{y}$;} \\
		& {\tt 7: $w^7$ = $R^7_{o2.f}$;} \\
	\end{tabular}
	\caption{\label{fig:heapAccess} A trace.}
\end{figure}

To address this challenge, we enhance the constraint system with the requirement that heap objects that are dereferenced in field access statements before the racy events retain their original address in the predicted trace.
This achieves two guarantees: First, matching heap access events in the original trace are guaranteed to also match in the predicted trace. Second, candidate races in the original trace remain viable in the predicted trace as they still refer to same location. Third, sharing between threads of heap locations remains unchanged  because
each location is accessed by the same number of threads given that the base object is the same. 





%Note that, the resolution of the virtual method call is modeled as the branch events that check the type of the target object, for which the constraints naturally guarantee the branch to take the same decision, i.e., the call to be resolved to the same method implementation.

%Finally, in a practical setting involving virtual method calls, call-site resolutions are the same across the original and predicted traces. 
%Notice that in this setting, we consider as relevant not only the targets of field dereferences but also the targets of virtual method invocations.

Formally, we require that
$$
\bigwedge_{ e_k \ has\  x.f \wedge e_i \in \tau\downarrow_{\leq i} }\
	x \equiv  \lsyn x \rsyn^{e_k}
$$
%where ${\sf env}$ is the state mapping from local variables to their value and $\sigma(t,\tau'')$ 
%(resp. $\sigma(t',\tau'')$) is the state arising at trace $t$ (resp. $t'$) immediately before event $\tau''$.
This constraint fixes that all heap dereferences  prior to the racy event retain their original base object as in the original trace $\tau$. 
In general, we need to specify such heap constraints when we hardcode the local heap accesses as local variables (Section~\ref{sec:ssa}) to ensure that the hardcoded data flow remains valid  after the relaxation. However,  most local heap accesses in the form of $x.f$ never read from any shared locations according to the static analysis, and therefore, cannot be affected by the relaxation. As an optimization, we do not specify the heap constraints for such local heap accesses.



By sending the above constraints to a solver, we compute the necessary schedule orders among the events as well as the mapping of the variables. The necessary schedule orders define a partial order among the events, which permit a set of schedules that define the complete order that complies with it.


